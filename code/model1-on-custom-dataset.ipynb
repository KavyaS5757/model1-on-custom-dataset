{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-08T21:25:59.140323Z","iopub.execute_input":"2023-06-08T21:25:59.140654Z","iopub.status.idle":"2023-06-08T21:25:59.155388Z","shell.execute_reply.started":"2023-06-08T21:25:59.140627Z","shell.execute_reply":"2023-06-08T21:25:59.154120Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"/kaggle/input/dataset/bed_room/bed-4343379__340.jpg\n/kaggle/input/dataset/bed_room/bed-4065946__340.jpg\n/kaggle/input/dataset/bed_room/bed-1846251__340.jpg\n/kaggle/input/dataset/bed_room/bed-4540208__340.jpg\n/kaggle/input/dataset/bed_room/bedroom-690129__340.jpg\n/kaggle/input/dataset/bed_room/bedroom-3102376__340.jpg\n/kaggle/input/dataset/bed_room/bed-1303451__340.jpg\n/kaggle/input/dataset/bed_room/bedroom-460762__340.jpg\n/kaggle/input/dataset/bed_room/bed-4343382__340.jpg\n/kaggle/input/dataset/bed_room/bedroom-4072391__340 (1).jpg\n/kaggle/input/dataset/bed_room/bedroom-374982__340.jpg\n/kaggle/input/dataset/bed_room/bed-3786264__340.jpg\n/kaggle/input/dataset/living_room/pexels-photo-279719.jpeg\n/kaggle/input/dataset/living_room/kitchen-3690727__340.jpg\n/kaggle/input/dataset/living_room/living-room-2732939__340.jpg\n/kaggle/input/dataset/living_room/living-living-room-1644496__340.jpg\n/kaggle/input/dataset/living_room/dining-room-332207__340.jpg\n/kaggle/input/dataset/living_room/kitchen-living-room-4043091__340.jpg\n/kaggle/input/dataset/living_room/kitchen-2165756__340.jpg\n/kaggle/input/dataset/living_room/living-room-2569325__340.jpg\n/kaggle/input/dataset/living_room/pexels-photo-276583.jpeg\n/kaggle/input/dataset/living_room/house-2563735__340.jpg\n/kaggle/input/dataset/living_room/interior-2685521__340.jpg\n/kaggle/input/dataset/living_room/pexels-photo-275484.jpeg\n/kaggle/input/dataset/living_room/living-room-670240__340.jpg\n/kaggle/input/dataset/living_room/living-room-690174__340.jpg\n/kaggle/input/dataset/living_room/living-room-728732__340.jpg\n/kaggle/input/dataset/living_room/living-room-3164434__340.jpg\n/kaggle/input/dataset/living_room/living-room-1048191__340.jpg\n/kaggle/input/dataset/living_room/living-room-2037945__340.jpg\n/kaggle/input/dataset/living_room/living-room-1523480__340.jpg\n/kaggle/input/dataset/living_room/pexels-photo-276724.jpeg\n/kaggle/input/dataset/living_room/furniture-998265__340.jpg\n/kaggle/input/dataset/living_room/pexels-photo-245208.jpeg\n/kaggle/input/dataset/living_room/living-room-2155376__340.jpg\n/kaggle/input/dataset/living_room/couch-1835923__340.jpg\n/kaggle/input/dataset/dining_room/centerpiece-714019__340.jpg\n/kaggle/input/dataset/dining_room/architecture-3214528__340.jpg\n/kaggle/input/dataset/dining_room/architectural-224242__340.jpg\n/kaggle/input/dataset/dining_room/chair-3321246__340.jpg\n/kaggle/input/dataset/dining_room/candles-126159__340.jpg\n/kaggle/input/dataset/dining_room/apartment-185779__340.jpg\n/kaggle/input/dataset/dining_room/dining-2112653__340.jpg\n/kaggle/input/dataset/dining_room/cafeteria-544871__340.jpg\n/kaggle/input/dataset/dining_room/chair-3306118__340.jpg\n/kaggle/input/dataset/dining_room/apartment-185778__340.jpg\n/kaggle/input/dataset/dining_room/apartment-2094648__340.jpg\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\n\ndataset_path = os.listdir('../input/dataset/')\n\nroom_types = os.listdir('../input/dataset/')\nprint(room_types)\n\nprint(\"Types of rooms found: \", len(dataset_path))","metadata":{"execution":{"iopub.status.busy":"2023-06-08T21:25:59.157276Z","iopub.execute_input":"2023-06-08T21:25:59.158563Z","iopub.status.idle":"2023-06-08T21:25:59.164437Z","shell.execute_reply.started":"2023-06-08T21:25:59.158515Z","shell.execute_reply":"2023-06-08T21:25:59.163765Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"['bed_room', 'living_room', 'dining_room']\nTypes of rooms found:  3\n","output_type":"stream"}]},{"cell_type":"code","source":"rooms = []\n\nfor item in room_types:\n    all_rooms = os.listdir('../input/dataset' + '/' + item)\n    \n    for room in all_rooms:\n        rooms.append((item, str('dataset' + '/' + item) + '/' + room))\n        print(rooms[:1])","metadata":{"execution":{"iopub.status.busy":"2023-06-08T21:25:59.165818Z","iopub.execute_input":"2023-06-08T21:25:59.166392Z","iopub.status.idle":"2023-06-08T21:25:59.181743Z","shell.execute_reply.started":"2023-06-08T21:25:59.166369Z","shell.execute_reply":"2023-06-08T21:25:59.180773Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n[('bed_room', 'dataset/bed_room/bed-4343379__340.jpg')]\n","output_type":"stream"}]},{"cell_type":"code","source":"rooms_df = pd.DataFrame(data=rooms, columns=['room type', 'image'])\nprint(rooms_df.head())\nprint(rooms_df.tail())","metadata":{"execution":{"iopub.status.busy":"2023-06-08T21:25:59.183555Z","iopub.execute_input":"2023-06-08T21:25:59.184454Z","iopub.status.idle":"2023-06-08T21:25:59.201942Z","shell.execute_reply.started":"2023-06-08T21:25:59.184420Z","shell.execute_reply":"2023-06-08T21:25:59.200866Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"  room type                                     image\n0  bed_room     dataset/bed_room/bed-4343379__340.jpg\n1  bed_room     dataset/bed_room/bed-4065946__340.jpg\n2  bed_room     dataset/bed_room/bed-1846251__340.jpg\n3  bed_room     dataset/bed_room/bed-4540208__340.jpg\n4  bed_room  dataset/bed_room/bedroom-690129__340.jpg\n      room type                                           image\n42  dining_room     dataset/dining_room/dining-2112653__340.jpg\n43  dining_room   dataset/dining_room/cafeteria-544871__340.jpg\n44  dining_room      dataset/dining_room/chair-3306118__340.jpg\n45  dining_room   dataset/dining_room/apartment-185778__340.jpg\n46  dining_room  dataset/dining_room/apartment-2094648__340.jpg\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Total number of rooms in the dataset: \", len(rooms_df))","metadata":{"execution":{"iopub.status.busy":"2023-06-08T21:25:59.204030Z","iopub.execute_input":"2023-06-08T21:25:59.204886Z","iopub.status.idle":"2023-06-08T21:25:59.219171Z","shell.execute_reply.started":"2023-06-08T21:25:59.204856Z","shell.execute_reply":"2023-06-08T21:25:59.218178Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Total number of rooms in the dataset:  47\n","output_type":"stream"}]},{"cell_type":"code","source":"room_count = rooms_df['room type'].value_counts()\nprint(\"rooms in each category\")\nprint(room_count)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T21:25:59.221643Z","iopub.execute_input":"2023-06-08T21:25:59.222106Z","iopub.status.idle":"2023-06-08T21:25:59.234249Z","shell.execute_reply.started":"2023-06-08T21:25:59.222072Z","shell.execute_reply":"2023-06-08T21:25:59.233224Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"rooms in each category\nliving_room    24\nbed_room       12\ndining_room    11\nName: room type, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"import cv2\npath = '../input/dataset/'\n\nim_size = 60\n\nimages = []\nlabels = []\n\nfor i in room_types:\n    data_path = path + str(i)\n    filenames = [i for i in os.listdir(data_path)]\n    \n    for f in filenames:\n        img = cv2.imread(data_path + '/' + f)\n        img = cv2.resize(img, (im_size, im_size))\n        images.append(img)\n        labels.append(i)\n        ","metadata":{"execution":{"iopub.status.busy":"2023-06-08T21:25:59.235704Z","iopub.execute_input":"2023-06-08T21:25:59.236659Z","iopub.status.idle":"2023-06-08T21:25:59.474737Z","shell.execute_reply.started":"2023-06-08T21:25:59.236624Z","shell.execute_reply":"2023-06-08T21:25:59.474065Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"images = np.array(images)\nimages.shape","metadata":{"execution":{"iopub.status.busy":"2023-06-08T21:25:59.475558Z","iopub.execute_input":"2023-06-08T21:25:59.476204Z","iopub.status.idle":"2023-06-08T21:25:59.481256Z","shell.execute_reply.started":"2023-06-08T21:25:59.476059Z","shell.execute_reply":"2023-06-08T21:25:59.480617Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"(47, 60, 60, 3)"},"metadata":{}}]},{"cell_type":"code","source":"images = images.astype('float32')/255.0","metadata":{"execution":{"iopub.status.busy":"2023-06-08T21:25:59.482104Z","iopub.execute_input":"2023-06-08T21:25:59.482843Z","iopub.status.idle":"2023-06-08T21:25:59.494019Z","shell.execute_reply.started":"2023-06-08T21:25:59.482816Z","shell.execute_reply":"2023-06-08T21:25:59.493210Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"images.shape","metadata":{"execution":{"iopub.status.busy":"2023-06-08T21:25:59.496616Z","iopub.execute_input":"2023-06-08T21:25:59.496934Z","iopub.status.idle":"2023-06-08T21:25:59.510113Z","shell.execute_reply.started":"2023-06-08T21:25:59.496910Z","shell.execute_reply":"2023-06-08T21:25:59.509132Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"(47, 60, 60, 3)"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\ny=rooms_df['room type'].values\nprint(y[:5])","metadata":{"execution":{"iopub.status.busy":"2023-06-08T21:25:59.511519Z","iopub.execute_input":"2023-06-08T21:25:59.511820Z","iopub.status.idle":"2023-06-08T21:25:59.520899Z","shell.execute_reply.started":"2023-06-08T21:25:59.511798Z","shell.execute_reply":"2023-06-08T21:25:59.520099Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"['bed_room' 'bed_room' 'bed_room' 'bed_room' 'bed_room']\n","output_type":"stream"}]},{"cell_type":"code","source":"y_labelencoder = LabelEncoder ()\ny = y_labelencoder.fit_transform(y)\nprint(y)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T21:25:59.521764Z","iopub.execute_input":"2023-06-08T21:25:59.522019Z","iopub.status.idle":"2023-06-08T21:25:59.537248Z","shell.execute_reply.started":"2023-06-08T21:25:59.521997Z","shell.execute_reply":"2023-06-08T21:25:59.536463Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"[0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1\n 1 1 1 1 1 1 1 1 1 1]\n","output_type":"stream"}]},{"cell_type":"code","source":"\nfrom sklearn.compose import ColumnTransformer\ny=y.reshape(-1,1)\nct = ColumnTransformer([(\"Country\", OneHotEncoder(), [0])], remainder = 'passthrough')\nY= ct.fit_transform(y)\nY.shape  #(393, 3)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T21:25:59.538118Z","iopub.execute_input":"2023-06-08T21:25:59.538384Z","iopub.status.idle":"2023-06-08T21:25:59.554178Z","shell.execute_reply.started":"2023-06-08T21:25:59.538362Z","shell.execute_reply":"2023-06-08T21:25:59.553478Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"(47, 3)"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\n\nimages, Y = shuffle(images, Y, random_state=1)\n\ntrain_x, test_x, train_y, test_y = train_test_split(images, Y, test_size = 0.05, random_state=415)\n\nprint(train_x.shape)\nprint(train_y.shape)\nprint(test_x.shape)\nprint(test_y.shape)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T21:25:59.555224Z","iopub.execute_input":"2023-06-08T21:25:59.556028Z","iopub.status.idle":"2023-06-08T21:25:59.565459Z","shell.execute_reply.started":"2023-06-08T21:25:59.556002Z","shell.execute_reply":"2023-06-08T21:25:59.564764Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"(44, 60, 60, 3)\n(44, 3)\n(3, 60, 60, 3)\n(3, 3)\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\n# Number of classes, one class for each of 5 flower.\nnum_classes = 3\n\n# flattened imge\nn_input = 10800\n\n\n# architecture hyper-parameter\nlearning_rate = 0.001\ntraining_iters = 10\nbatch_size = 16\ndisplay_step = 20","metadata":{"execution":{"iopub.status.busy":"2023-06-08T21:25:59.566743Z","iopub.execute_input":"2023-06-08T21:25:59.567195Z","iopub.status.idle":"2023-06-08T21:25:59.577497Z","shell.execute_reply.started":"2023-06-08T21:25:59.567170Z","shell.execute_reply":"2023-06-08T21:25:59.576397Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"import tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\nimg_size=60\nnum_channels=3\n#x = tf.placeholder(tf.float32, [None, n_input])\nx = tf.placeholder(tf.float32, shape=[None, img_size,img_size,num_channels])  # None,60,60,3\n\ny_ = tf.placeholder(tf.float32, [None, num_classes]) # None,3\n\n\n\nprint('Shape of placeholder',x.shape, y_.shape)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T21:25:59.578597Z","iopub.execute_input":"2023-06-08T21:25:59.579069Z","iopub.status.idle":"2023-06-08T21:25:59.595467Z","shell.execute_reply.started":"2023-06-08T21:25:59.579039Z","shell.execute_reply":"2023-06-08T21:25:59.593780Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"Shape of placeholder (?, 60, 60, 3) (?, 3)\n","output_type":"stream"}]},{"cell_type":"code","source":"def conv2d(x, W, b, strides=1):\n    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')   # summation = wx+b, Activation\n    x = tf.nn.bias_add(x, b)\n    return tf.nn.relu(x)\n\ndef maxpool2d(x, k=2):\n    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')","metadata":{"execution":{"iopub.status.busy":"2023-06-08T21:25:59.597324Z","iopub.execute_input":"2023-06-08T21:25:59.597611Z","iopub.status.idle":"2023-06-08T21:25:59.608672Z","shell.execute_reply.started":"2023-06-08T21:25:59.597586Z","shell.execute_reply":"2023-06-08T21:25:59.607781Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"weights = {\n    'w1': tf.Variable(tf.random_normal([5, 5, 3, 32]),name='w1'),\n    'w2': tf.Variable(tf.random_normal([5, 5, 32, 64]),name='w2'),\n    'w3': tf.Variable(tf.random_normal([5, 5, 64, 128]),name='w3'),\n    'wd1': tf.Variable(tf.random_normal([8 * 8 * 128, 2048]),name='wd1'),  \n    'wout': tf.Variable(tf.random_normal([2048, num_classes]),name='wout')\n}\n\nbiases = {\n    'b1': tf.Variable(tf.random_normal([32]),name='b1'),\n    'b2': tf.Variable(tf.random_normal([64]),name='b2'),\n    'b3': tf.Variable(tf.random_normal([128]),name='b3'),\n    'bd1': tf.Variable(tf.random_normal([2048]),name='bd1'),\n    'bout': tf.Variable(tf.random_normal([num_classes]),name='bout')\n}","metadata":{"execution":{"iopub.status.busy":"2023-06-08T21:25:59.610119Z","iopub.execute_input":"2023-06-08T21:25:59.610796Z","iopub.status.idle":"2023-06-08T21:25:59.676089Z","shell.execute_reply.started":"2023-06-08T21:25:59.610763Z","shell.execute_reply":"2023-06-08T21:25:59.675379Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"def conv_net(x, weights, biases):\n        \n    # reshape input to 60x60x3 size\n    x = tf.reshape(x, shape=[-1, 60, 60, 3])  \n    \n    print(\"###########################################################################\")\n    print(\"size of x is\")\n    print(x.shape)\n    \n  \n    conv1 = conv2d(x, weights['w1'], biases['b1'])\n    conv1 = maxpool2d(conv1, k=2)\n    print(\"###########################################################################\")\n    print(\"size after 1st conv layer is \")\n    print(conv1.shape)\n\n    \n    #input is 30*30*32 image\n    # Convolution Layer\n    conv2 = conv2d(conv1, weights['w2'], biases['b2'])\n    conv2 = maxpool2d(conv2, k=2)\n    print(\"###########################################################################\")\n    print(\"size after 2nd conv and pooling layer is\")\n    print(conv2.shape)\n    \n     ### third conv layer\n    # input is 15*15*64 image\n    # Convolution Layer\n    conv3 = conv2d(conv2, weights['w3'], biases['b3'])\n  \n    conv3 = maxpool2d(conv3, k=2)\n    print(\"###########################################################################\")\n    print(\"size after 3rd conv and pooling layer is\")\n    print(conv3.shape)\n    \n    \n    \n    #input is 8*8*128 \n\n    # Fully connected layer\n    # Reshape conv3 output to fit fully connected layer input   = 8*8*128 = 8192\n    fc1 = tf.reshape(conv3, [-1, weights['wd1'].get_shape().as_list()[0]])\n    print(\"###########################################################################\")\n    print(\"shape after flattening the image\")\n    print(fc1)  #8192 is the output\n    \n    \n    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n    fc1 = tf.nn.relu(fc1)\n    print(\"###########################################################################\")\n    print(\"shape after fully connected layer\")\n    print(fc1)\n    \n    # Output, class prediction\n    # finally we multiply the fully connected layer with the weights and add a bias term. \n    out = tf.add(tf.matmul(fc1, weights['wout']), biases['bout'])\n    print(\"###########################################################################\")\n    print(\"Output layer\")\n    return out","metadata":{"execution":{"iopub.status.busy":"2023-06-08T21:25:59.677025Z","iopub.execute_input":"2023-06-08T21:25:59.677576Z","iopub.status.idle":"2023-06-08T21:25:59.686874Z","shell.execute_reply.started":"2023-06-08T21:25:59.677552Z","shell.execute_reply":"2023-06-08T21:25:59.685857Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"# Create the model\nmodel = conv_net(x, weights, biases)\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T21:25:59.690845Z","iopub.execute_input":"2023-06-08T21:25:59.691175Z","iopub.status.idle":"2023-06-08T21:25:59.727073Z","shell.execute_reply.started":"2023-06-08T21:25:59.691152Z","shell.execute_reply":"2023-06-08T21:25:59.726377Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"###########################################################################\nsize of x is\n(?, 60, 60, 3)\n###########################################################################\nsize after 1st conv layer is \n(?, 30, 30, 32)\n###########################################################################\nsize after 2nd conv and pooling layer is\n(?, 15, 15, 64)\n###########################################################################\nsize after 3rd conv and pooling layer is\n(?, 8, 8, 128)\n###########################################################################\nshape after flattening the image\nTensor(\"Reshape_3:0\", shape=(?, 8192), dtype=float32)\n###########################################################################\nshape after fully connected layer\nTensor(\"Relu_7:0\", shape=(?, 2048), dtype=float32)\n###########################################################################\nOutput layer\nTensor(\"Add_3:0\", shape=(?, 3), dtype=float32)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define loss and optimizer\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=y_))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T21:25:59.729552Z","iopub.execute_input":"2023-06-08T21:25:59.729940Z","iopub.status.idle":"2023-06-08T21:25:59.905482Z","shell.execute_reply.started":"2023-06-08T21:25:59.729916Z","shell.execute_reply":"2023-06-08T21:25:59.904796Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"# Initializing the variables\ninit = tf.global_variables_initializer()","metadata":{"execution":{"iopub.status.busy":"2023-06-08T21:25:59.906471Z","iopub.execute_input":"2023-06-08T21:25:59.906847Z","iopub.status.idle":"2023-06-08T21:25:59.913314Z","shell.execute_reply.started":"2023-06-08T21:25:59.906817Z","shell.execute_reply":"2023-06-08T21:25:59.912163Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"cost_history=[]\nn_epochs =10\n# the execution\nsess = tf.Session()\nsess.run(init)\n\ntrain_y=np.asarray(train_y).tolist()\n\nfor i in range(n_epochs):\n    a, c = sess.run([optimizer, cost], feed_dict={x: train_x, y_: train_y})  #working\n    cost_history = np.append(cost_history,c)  # working\n    print('epoch : ', i,  ' - ', 'cost: ', c) #working","metadata":{"execution":{"iopub.status.busy":"2023-06-08T21:30:03.186360Z","iopub.execute_input":"2023-06-08T21:30:03.186678Z","iopub.status.idle":"2023-06-08T21:30:08.048447Z","shell.execute_reply.started":"2023-06-08T21:30:03.186652Z","shell.execute_reply":"2023-06-08T21:30:08.047767Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"epoch :  0  -  cost:  1711474.9\nepoch :  1  -  cost:  3609766.5\nepoch :  2  -  cost:  3753503.0\nepoch :  3  -  cost:  3273619.2\nepoch :  4  -  cost:  2676799.0\nepoch :  5  -  cost:  1635069.9\nepoch :  6  -  cost:  547124.75\nepoch :  7  -  cost:  247498.86\nepoch :  8  -  cost:  1357790.5\nepoch :  9  -  cost:  927838.0\n","output_type":"stream"}]},{"cell_type":"code","source":"test_y=np.asarray(test_y).tolist()  #working solution of ValueError: setting an array element with a sequence.\n#print(test_y)\n\ncorrect_prediction = tf.equal(tf.argmax(model,1), tf.argmax(y_,1))   \ncorrect_prediction \n\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\naccuracy\n\n\n# retrun the accuracy on the test set.\nprint(\"Accuracy: \", sess.run(accuracy, feed_dict={x: test_x, y_:test_y}))","metadata":{"execution":{"iopub.status.busy":"2023-06-08T21:31:10.788475Z","iopub.execute_input":"2023-06-08T21:31:10.788863Z","iopub.status.idle":"2023-06-08T21:31:10.849999Z","shell.execute_reply.started":"2023-06-08T21:31:10.788835Z","shell.execute_reply":"2023-06-08T21:31:10.849196Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"Accuracy:  0.33333334\n","output_type":"stream"}]}]}